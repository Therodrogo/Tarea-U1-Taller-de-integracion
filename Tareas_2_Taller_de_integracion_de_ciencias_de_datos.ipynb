{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPulS6/wwjEucP3F99Aod2f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Therodrogo/Tarea-U1-Taller-de-integracion/blob/main/Tareas_2_Taller_de_integracion_de_ciencias_de_datos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MHEALTH: descarga, carga, exploración y preprocesamiento\n"
      ],
      "metadata": {
        "id": "4rtPGonUxGan"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descarga y extración del dataset"
      ],
      "metadata": {
        "id": "PRJSXw0HxBt3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LlHuTOnXgLDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e98c3db-adf0-4f35-e150-211ff7bcb093"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descargando desde: https://archive.ics.uci.edu/static/public/319/mhealth+dataset.zip\n",
            "Descarga OK.\n",
            "Archivos extraídos en: /content/mhealth\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import io\n",
        "import zipfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "DATA_DIR = Path(\"mhealth\")\n",
        "DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "zip_bytes = None\n",
        "last_error = None\n",
        "\n",
        "url=  \"https://archive.ics.uci.edu/static/public/319/mhealth+dataset.zip\"\n",
        "\n",
        "try:\n",
        "    print(f\"Descargando desde: {url}\")\n",
        "    r = requests.get(url, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    zip_bytes = r.content\n",
        "    print(\"Descarga OK.\")\n",
        "except Exception as e:\n",
        "    print(f\"Fallo con {url}: {e}\")\n",
        "    last_error = e\n",
        "\n",
        "if zip_bytes is None:\n",
        "    raise RuntimeError(f\"No se pudo descargar el ZIP desde UCI. Último error: {last_error}\")\n",
        "\n",
        "# Extraer\n",
        "with zipfile.ZipFile(io.BytesIO(zip_bytes)) as zf:\n",
        "    zf.extractall(DATA_DIR)\n",
        "print(f\"Archivos extraídos en: {DATA_DIR.resolve()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Se verifica que se hayan descargado correctamente"
      ],
      "metadata": {
        "id": "Lfjn6OmwzI4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_files = sorted([p for p in DATA_DIR.rglob(\"*\") if re.search(r\"mHealth_subject\\d+\\.log$\", p.name, re.I)])\n",
        "\n",
        "if not log_files:\n",
        "    raise RuntimeError(\n",
        "        \"No se encontraron archivos 'mHealth_subject*.log' tras extraer el ZIP. \"\n",
        "        \"Revisa el contenido de la carpeta 'mhealth/'.\"\n",
        "    )\n",
        "\n",
        "print(f\"Archivos de sujetos encontrados: {len(log_files)}\")\n",
        "for p in log_files[:5]:\n",
        "    print(\" -\", p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3AjtT8azNQL",
        "outputId": "f79b12ed-224c-40f2-a285-372d7bb9a026"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivos de sujetos encontrados: 10\n",
            " - mhealth/MHEALTHDATASET/mHealth_subject1.log\n",
            " - mhealth/MHEALTHDATASET/mHealth_subject10.log\n",
            " - mhealth/MHEALTHDATASET/mHealth_subject2.log\n",
            " - mhealth/MHEALTHDATASET/mHealth_subject3.log\n",
            " - mhealth/MHEALTHDATASET/mHealth_subject4.log\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definición de columnas del dataset MHEALTH\n",
        "\n",
        "A partir del archivo `README.txt`, se definen las 24 columnas del dataset.  \n",
        "En la siguiente tabla se muestra la abreviación utilizada y su descripción:\n",
        "\n",
        "| Nº Columna | Abreviación   | Descripción |\n",
        "|------------|--------------|-------------|\n",
        "| 1          | acc_chest_x  | Aceleración desde el sensor del pecho (eje X) |\n",
        "| 2          | acc_chest_y  | Aceleración desde el sensor del pecho (eje Y) |\n",
        "| 3          | acc_chest_z  | Aceleración desde el sensor del pecho (eje Z) |\n",
        "| 4          | ecg_1        | Señal de electrocardiograma (lead 1) |\n",
        "| 5          | ecg_2        | Señal de electrocardiograma (lead 2) |\n",
        "| 6          | acc_ankle_x  | Aceleración desde el sensor del tobillo izquierdo (eje X) |\n",
        "| 7          | acc_ankle_y  | Aceleración desde el sensor del tobillo izquierdo (eje Y) |\n",
        "| 8          | acc_ankle_z  | Aceleración desde el sensor del tobillo izquierdo (eje Z) |\n",
        "| 9          | gyro_ankle_x | Giroscopio desde el sensor del tobillo izquierdo (eje X) |\n",
        "| 10         | gyro_ankle_y | Giroscopio desde el sensor del tobillo izquierdo (eje Y) |\n",
        "| 11         | gyro_ankle_z | Giroscopio desde el sensor del tobillo izquierdo (eje Z) |\n",
        "| 12         | mag_ankle_x  | Magnetómetro desde el sensor del tobillo izquierdo (eje X) |\n",
        "| 13         | mag_ankle_y  | Magnetómetro desde el sensor del tobillo izquierdo (eje Y) |\n",
        "| 14         | mag_ankle_z  | Magnetómetro desde el sensor del tobillo izquierdo (eje Z) |\n",
        "| 15         | acc_arm_x    | Aceleración desde el sensor del brazo derecho (eje X) |\n",
        "| 16         | acc_arm_y    | Aceleración desde el sensor del brazo derecho (eje Y) |\n",
        "| 17         | acc_arm_z    | Aceleración desde el sensor del brazo derecho (eje Z) |\n",
        "| 18         | gyro_arm_x   | Giroscopio desde el sensor del brazo derecho (eje X) |\n",
        "| 19         | gyro_arm_y   | Giroscopio desde el sensor del brazo derecho (eje Y) |\n",
        "| 20         | gyro_arm_z   | Giroscopio desde el sensor del brazo derecho (eje Z) |\n",
        "| 21         | mag_arm_x    | Magnetómetro desde el sensor del brazo derecho (eje X) |\n",
        "| 22         | mag_arm_y    | Magnetómetro desde el sensor del brazo derecho (eje Y) |\n",
        "| 23         | mag_arm_z    | Magnetómetro desde el sensor del brazo derecho (eje Z) |\n",
        "| 24         | label        | Etiqueta de actividad (0 corresponde a la clase nula) |\n"
      ],
      "metadata": {
        "id": "a4Sx2kLvzaM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\n",
        "    \"acc_chest_x\", \"acc_chest_y\", \"acc_chest_z\",\n",
        "    \"ecg_1\", \"ecg_2\",\n",
        "    \"acc_ankle_x\", \"acc_ankle_y\", \"acc_ankle_z\",\n",
        "    \"gyro_ankle_x\", \"gyro_ankle_y\", \"gyro_ankle_z\",\n",
        "    \"mag_ankle_x\", \"mag_ankle_y\", \"mag_ankle_z\",\n",
        "    \"acc_arm_x\", \"acc_arm_y\", \"acc_arm_z\",\n",
        "    \"gyro_arm_x\", \"gyro_arm_y\", \"gyro_arm_z\",\n",
        "    \"mag_arm_x\", \"mag_arm_y\", \"mag_arm_z\",\n",
        "    \"label\"\n",
        "]\n",
        "\n",
        "#Cargar todos los sujetos y concatenar\n",
        "\n",
        "dfs = []\n",
        "for fp in log_files:\n",
        "\n",
        "    # subject_id desde el nombre del archivo\n",
        "    m = re.search(r\"subject(\\d+)\", fp.name, re.I)  #Usa expresiones regulares para buscar un número en el nombre del archivo\n",
        "    subject_id = int(m.group(1)) if m else -1\n",
        "\n",
        "    #Los .log no traen nombres de columnas, solo valores.\n",
        "    #Los valores están separados por espacios (no por coma, por eso se usa sep=r\"\\s+\").\n",
        "    #header=None → para que pandas no use la primera fila como nombre de columna.\n",
        "    df_subj = pd.read_csv(\n",
        "        fp,\n",
        "        sep=r\"\\s+\",\n",
        "        header=None,\n",
        "        engine=\"python\"\n",
        "    )\n",
        "\n",
        "    # Validación de columnas esperadas\n",
        "    if df_subj.shape[1] != 24:\n",
        "        raise ValueError(f\"{fp.name} tiene {df_subj.shape[1]} columnas; se esperaban 24.\")\n",
        "\n",
        "    #Ahora cada columna tiene un nombre significativo.\n",
        "    #Se agrega una columna extra \"subject_id\" para saber a qué persona corresponde cada fila.\n",
        "    df_subj.columns = columns\n",
        "    df_subj[\"subject_id\"] = subject_id\n",
        "\n",
        "    #Guardamos los datos de este sujeto en la lista general dfs\n",
        "    dfs.append(df_subj)\n",
        "\n",
        "#Unimos todos los df_subj en un solo DataFrame gigante.\n",
        "#ignore_index=True reinicia los índices (para que no queden duplicados).\n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "print(\"\\n=== Dimensiones del DataFrame completo ===\")\n",
        "print(df.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-MrSM8g1eC-",
        "outputId": "ab3c8059-76c0-4cc2-c26b-236db85e285d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Dimensiones del DataFrame completo ===\n",
            "(1215745, 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploracion de datos"
      ],
      "metadata": {
        "id": "0HhPodLI9DUJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploración inicial de los datos\n",
        "\n",
        "### 1. Tipos de datos\n",
        "Se revisan los tipos de datos de cada columna para verificar que las señales estén en formato numérico (`float` o `int`) y que la columna de etiquetas (`label`) tenga un tipo adecuado. Esto es importante porque, si alguna columna está mal cargada (por ejemplo como `object`), podría dar problemas en el análisis.\n",
        "\n",
        "### 2. Valores faltantes\n",
        "Se cuentan los valores faltantes (`NaN`) por columna, ordenados de mayor a menor.  \n",
        "Este paso nos ayuda a identificar si existen columnas con problemas de calidad de datos que requieren imputar, eliminación o tratamiento especial.\n",
        "\n",
        "### 3. Estadísticas descriptivas\n",
        "Se calculan medidas estadísticas básicas (media, desviación estándar, mínimo, máximo, percentiles) de las variables numéricas.  \n",
        "Con esto podemos:\n",
        "- Identificar escalas diferentes entre sensores.\n",
        "- Detectar posibles valores atípicos (por ejemplo, lecturas de sensores fuera de rango esperado).\n",
        "- Ver la dispersión de los datos.\n",
        "\n",
        "### 4. Distribución de etiquetas\n",
        "Se cuenta cuántos registros tiene cada etiqueta de actividad (`label`).  \n",
        "Esto permite analizar si el dataset está balanceado o si hay actividades con muchas más observaciones que otras, lo cual es importante para un futuro modelo de clasificación.\n",
        "\n",
        "### 5. Visualizaciones iniciales\n",
        "- **Histograma de `acc_chest_x`**: Permite observar la distribución de valores de una señal de acelerómetro ubicada en el pecho, identificando posibles asimetrías o valores atipicos.  \n",
        "- **Gráfico de barras de la distribución de clases (`label`)**: Muestra de manera visual la cantidad de registros por cada clase, lo que complementa el análisis numérico previo de balance de clases.\n",
        "\n",
        "Estas visualizaciones ayudan a tener una primera intuición sobre la calidad y características del dataset."
      ],
      "metadata": {
        "id": "YdeooJVZ_ZcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Esto muestra el tipo de cada columna en el DataFrame (int64, float64, object, etc.)\n",
        "print(\"\\n=== Tipos de datos ===\")\n",
        "print(df.dtypes)\n",
        "\n",
        "#Cuenta cuántos valores faltan (NaN) por columna, ordena de mayor a menor y muestra las 10 columnas con más valores faltantes.\n",
        "print(\"\\n=== Valores faltantes (top 10) ===\")\n",
        "print(df.isna().sum().sort_values(ascending=False).head(10))\n",
        "\n",
        "#Aqui mostramos la media, desviación estándar, min, max, percentiles para las columnas numéricas.\n",
        "print(\"\\n=== Estadísticas descriptivas (primeras columnas) ===\")\n",
        "print(df.describe().T.head(10))\n",
        "\n",
        "#Esto cuenta cuántos registros tiene cada etiqueta.\n",
        "print(\"\\n=== Distribución de la etiqueta (label) ===\")\n",
        "print(df[\"label\"].value_counts().sort_index())\n",
        "\n",
        "#Visualizaciones iniciales\n",
        "try:\n",
        "    #Histograma de una señal ejemplo (aceleración pecho X)\n",
        "    #Esto sirve para ver cómo se distribuyen los valores de esa variable (¿normal? ¿asimétrica? ¿hay valores atipicos?).\n",
        "    plt.figure()\n",
        "    df[\"acc_chest_x\"].hist(bins=60)\n",
        "    plt.title(\"Distribución acc_chest_x\")\n",
        "    plt.xlabel(\"m/s^2\")\n",
        "    plt.ylabel(\"Frecuencia\")\n",
        "    plt.show()\n",
        "\n",
        "    #Conteo de clases\n",
        "    #Se muestra visualmente si hay clases con muchas o pocas muestras.\n",
        "    plt.figure()\n",
        "    df[\"label\"].value_counts().sort_index().plot(kind=\"bar\")\n",
        "    plt.title(\"Distribución de etiquetas (0 = null class)\")\n",
        "    plt.xlabel(\"Etiqueta\")\n",
        "    plt.ylabel(\"Frecuencia\")\n",
        "    plt.show()\n",
        "except Exception as _:\n",
        "    pass"
      ],
      "metadata": {
        "id": "Kz0bBdlm9Ghu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limpieza inicial de los datos\n",
        "\n",
        "### Eliminación de duplicados\n",
        "Se eliminaron filas duplicadas exactas del dataset para evitar que las observaciones repetidas afecten los análisis posteriores.  \n",
        "Esto asegura que cada fila corresponda a una medición única.\n",
        "\n",
        "### Manejo de valores nulos\n",
        "- **Señales numéricas**: si existen valores faltantes, se imputan mediante interpolación lineal.  \n",
        "  Esto es apropiado porque las señales fisiológicas son series temporales continuas y se pueden estimar entre valores vecinos.\n",
        "- **Etiqueta (`label`)**: si contiene nulos, se reemplazan con la moda.  \n",
        "  Esta estrategia es simple pero razonable cuando la cantidad de valores faltantes es baja.\n",
        "\n",
        "Después de este proceso, se verifica que no queden valores nulos en el dataset.\n",
        "\n",
        "### Manejo de valores atípicos\n",
        "Se implementa una técnica de **winsorización basada en el rango intercuartílico:\n",
        "- Se calculan los límites inferior y superior permitidos para cada variable.  \n",
        "- Los valores fuera de rango se recortan a esos límites.  \n",
        "- Se utilizó un factor `k=3.0`, más permisivo que el estándar (`1.5`), ya que los datos fisiológicos suelen tener variaciones amplias que no necesariamente representan errores.\n",
        "\n",
        "Con este paso, se atenúan los valos atipicos extremos sin eliminar información valiosa.\n"
      ],
      "metadata": {
        "id": "Ro6pB6vNBYgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Eliminar duplicados exactos (si los hubiera)\n",
        "before = len(df)                      # número de filas antes de limpiar\n",
        "df = df.drop_duplicates()             # elimina filas duplicadas exactas\n",
        "after = len(df)                       # número de filas después de limpiar\n",
        "print(f\"\\nDuplicados eliminados: {before - after}\")  # muestra cuántos se eliminaron\n",
        "\n",
        "#Manejo de valores nulos\n",
        "#Separamos columnas de señales (todos menos 'label' y 'subject_id')\n",
        "signals_cols = [c for c in df.columns if c not in (\"label\", \"subject_id\")]\n",
        "\n",
        "#Para señales numéricas:\n",
        "#Si hay valores faltantes, se imputan mediante interpolación lineal\n",
        "if df[signals_cols].isna().sum().sum() > 0:\n",
        "    df[signals_cols] = df[signals_cols].interpolate(method=\"linear\", limit_direction=\"both\")\n",
        "\n",
        "#Para la columna 'label':\n",
        "#Si existen nulos, se reemplazan por la moda (el valor más frecuente)\n",
        "if df[\"label\"].isna().any():\n",
        "    moda_label = df[\"label\"].mode().iloc[0]\n",
        "    df[\"label\"] = df[\"label\"].fillna(moda_label)\n",
        "\n",
        "print(\"\\nNulos restantes (total):\", int(df.isna().sum().sum()))  #total de nulos después de la limpieza\n",
        "\n",
        "#Detección y atenuación de valores atípicos mediante winsorización por IQR\n",
        "def winsorize_iqr(s, k=1.5):\n",
        "    #Calcula el rango intercuartílico\n",
        "    q1, q3 = s.quantile([0.25, 0.75])\n",
        "    iqr = q3 - q1\n",
        "    #Límites inferior y superior permitidos\n",
        "    low = q1 - k * iqr\n",
        "    high = q3 + k * iqr\n",
        "    #Recorta (clip) los valores fuera del rango\n",
        "    return s.clip(lower=low, upper=high)\n",
        "\n",
        "#Aplicamos winsorización a todas las columnas de señales\n",
        "df_w = df.copy()\n",
        "for c in signals_cols:\n",
        "    df_w[c] = winsorize_iqr(df_w[c], k=3.0)  # k=3 es más permisivo, útil para series fisiológicas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PoS3JzlBcB3",
        "outputId": "1657166b-c6c3-4165-f460-23cc49acf069"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Duplicados eliminados: 0\n",
            "\n",
            "Nulos restantes (total): 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estandarización\n",
        "\n",
        "Se aplicó **StandardScaler** a todas las columnas de señales para que tengan media = 0 y desviación estándar = 1.  \n",
        "Esto es opcional, pero útil para modelos de machine learning sensibles a la escala de los datos (por ejemplo SVM, KNN o redes neuronales).  \n",
        "No se escalaron las columnas `label` ni `subject_id`.\n",
        "\n",
        "## Guardado de resultados\n",
        "\n",
        "- `mhealth_raw.csv`: Dataset concatenado sin limpieza.  \n",
        "- `mhealth_clean.csv`: Dataset limpio, winsorizado y estandarizado.  \n",
        "\n",
        "Se guardan en formato CSV para facilitar el análisis posterior."
      ],
      "metadata": {
        "id": "ni8z2-y6Dbyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Estandarización\n",
        "df_scaled = df_w.copy()              # Creamos una copia del DataFrame ya limpiado y winsorizado\n",
        "scaler = StandardScaler()            # Inicializamos el escalador de sklearn\n",
        "# Aplicamos estandarización (media=0, desviación estándar=1) solo a las columnas de señales\n",
        "df_scaled[signals_cols] = scaler.fit_transform(df_scaled[signals_cols])\n",
        "\n",
        "#Guardar resultados\n",
        "RAW_CSV = \"mhealth_raw.csv\"          # Nombre del CSV con datos concatenados sin limpiar\n",
        "CLEAN_CSV = \"mhealth_clean.csv\"      # Nombre del CSV con datos limpios, winsorizados y estandarizados\n",
        "\n",
        "#Guardar DataFrames en archivos CSV\n",
        "df.to_csv(RAW_CSV, index=False)\n",
        "df_scaled.to_csv(CLEAN_CSV, index=False)\n",
        "\n",
        "print(f\"\\nGuardado: {RAW_CSV} (raw concatenado)\")\n",
        "print(f\"Guardado: {CLEAN_CSV} (limpio + winsor + estandarizado)\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jH2VLQQlhUaj",
        "outputId": "e7057668-43e7-44e4-d7df-ea8357c1e134"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Guardado: mhealth_raw.csv (raw concatenado)\n",
            "Guardado: mhealth_clean.csv (limpio + winsor + estandarizado)\n"
          ]
        }
      ]
    }
  ]
}